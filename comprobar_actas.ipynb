import requests
from bs4 import BeautifulSoup
from concurrent import futures
import time
from datetime import datetime
import operator
import pandas as pd
from google.colab import files

ESPERA = 3
SIMULTANEIDAD = 5
EXCEL = False
CSV = True

URL = "https://ceice.gva.es/auto/Actas/"
CABECERAS = {
    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/114.0',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
    'Accept-Language': 'es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'none',
    'Sec-Fetch-User': '?1',
    'Pragma': 'no-cache',
    'Cache-Control': 'no-cache',
}

def extraer_pdf(data):
  carpeta_nombre = data["carpeta_nombre"]
  carpeta_href = data["carpeta_href"]
  subcarpeta_nombre = data["subcarpeta_nombre"]
  subcarpeta_href = data["subcarpeta_href"]
  peticion = requests.get(f"{URL}{carpeta_href}/{subcarpeta_href}")
  if peticion.status_code != 200:
    return {"error": f"status code incorrecto {peticion.status_code} comprobando subcarpeta {subcarpeta_nombre}"}
  soup = BeautifulSoup(peticion.text, "html.parser")
  filas = soup.select("#indexlist tr:not(.indexhead):not(.indexbreakrow)")
  if filas == None:
    return {"error": f"no se han encontrado archivos pdf en {subcarpeta_nombre}: {URL}{subcarpeta_href}"}
  pdf = []
  for fila in filas:
    try:
      fila_archivo = fila.select_one("td.indexcolname a")
      pdf_nombre = fila_archivo.text
      pdf_href = fila_archivo.attrs['href']

      fecha_modificacion = fila.select_one("td.indexcollastmod").text.strip()
      fecha_formato = '%Y-%m-%d %H:%M'
      fecha = datetime.strptime(fecha_modificacion, fecha_formato)
      fecha_unix = int(time.mktime(fecha.timetuple()))
      pdf.append({
          "carpeta_nombre": carpeta_nombre,
          "carpeta_href": f"{URL}{carpeta_href}",
          "subcarpeta_nombre": subcarpeta_nombre,
          "subcarpeta_href": f"{URL}{carpeta_href}/{subcarpeta_href}",
          "pdf_nombre": pdf_nombre,
          "pdf_href": f"{URL}{carpeta_href}/{subcarpeta_href}/{pdf_href}",
          "fecha_modificacion": fecha_modificacion,
          "fecha_unix": fecha_unix
      })
    except Exception as error:
        print(f"error procesando {subcarpeta_nombre}: {error}")
  
  return {"data": pdf, "error": ""}
    

def extraer_subcarpetas(data):
  carpeta_href = data["carpeta_href"]
  carpeta_nombre = data["carpeta_nombre"]
  peticion = requests.get(f"{URL}{carpeta_href}", headers=CABECERAS)
  if peticion.status_code != 200:
    return {"error": f"status code incorrecto {peticion.status_code} comprobando {carpeta_nombre}"}
  soup = BeautifulSoup(peticion.text, "html.parser")
  subcarpetas = soup.select("tr:not(.indexhead) .indexcolname > a")
  if len(subcarpetas) == 0:
    return {"error": f"no se han encontrado subcarpetas en {carpeta_nombre}"}
  data = {
      "subcarpetas": []
  }
  for subcarpeta in subcarpetas:
    info_subcarpeta = {
      "carpeta_nombre": carpeta_nombre,
      "carpeta_href": carpeta_href,
      "subcarpeta_nombre": subcarpeta.text,
      "subcarpeta_href": subcarpeta.attrs["href"]
    }
    data["subcarpetas"].append(info_subcarpeta)
  return {"data": data, "error": ""}


def comprobar_actas():
  print("Inicio del script")
  peticion_raiz = requests.get(URL, headers=CABECERAS)
  if peticion_raiz.status_code != 200:
    print(f"status code incorrecto {peticion_raiz.status_code} comprobando {URL}")
    return
  soup = BeautifulSoup(peticion_raiz.text, "html.parser")

  ## Carpetas
  carpetas_enlaces = soup.select("tr:not(.indexhead) .indexcolname > a")
  if len(carpetas_enlaces) == 0:
    print(f"no se han encontrado carpetas en {URL}")

  carpetas = []
  for carpeta_enlaces in carpetas_enlaces:
    info = {
        "carpeta_nombre": carpeta_enlaces.text,
        "carpeta_href": carpeta_enlaces.attrs["href"]
    }
    carpetas.append(info)

  print(f"Se han encontrado {len(carpetas)} carpetas en la página principal. Por favor, espere mientras continúa el análisis...")

  ## Subcarpetas
  subcarpetas = []
  with futures.ThreadPoolExecutor(max_workers=SIMULTANEIDAD) as ejecutor:
    respuestas = ejecutor.map(extraer_subcarpetas, carpetas)
    for respuesta in respuestas:
      if respuesta["error"] != "":
        print(respuesta["error"])
        continue
      for subcarpeta in respuesta["data"]["subcarpetas"]:
        subcarpetas.append(subcarpeta)
    time.sleep(ESPERA)
  
  print(f"Se han encontrado {len(subcarpetas)} subcarpetas dentro de las {len(carpetas)} carpetas. Por favor, espere mientras continúa el análisis...")

  ## Enlaces
  enlaces = []
  with futures.ThreadPoolExecutor(max_workers=SIMULTANEIDAD) as ejecutor:
    respuestas = ejecutor.map(extraer_pdf, subcarpetas)
    for respuesta in respuestas:
      if respuesta["error"] != "":
        print(respuesta["error"])
        continue
      for enlace in respuesta["data"]:
        enlaces.append(enlace)
    time.sleep(ESPERA)

  print(f"Se han encontrado {len(enlaces)} enlaces en las {len(subcarpetas)} subcarpetas. Por favor, espere mientras continúa el análisis...")

  enlaces.sort(key=operator.itemgetter('fecha_unix'), reverse=True)
  
  df = pd.DataFrame(data=enlaces)
  try:
    if EXCEL:
      df.to_excel("resultados_actas.xlsx", index=False)
      files.download("resultados_actas.xlsx")
    if CSV:
      df.to_csv("resultados_actas.csv", index=False)
      files.download("resultados_actas.csv")
  except Exception as error:
    print(f"Se ha producido un error guardando los archivos: {error}")
    print("Se va a mostrar por pantalla la información")
    for enlace in enlaces:
      print(enlace)

comprobar_actas()
